{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and fix PyArrow compatibility issues\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def check_and_fix_pyarrow():\n",
    "    \"\"\"Check PyArrow version and fix compatibility issues\"\"\"\n",
    "    print(\"ğŸ” Checking PyArrow compatibility...\")\n",
    "    \n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        print(f\"ğŸ“¦ PyArrow version: {pa.__version__}\")\n",
    "        \n",
    "        # Test for the specific attribute that's causing issues\n",
    "        if hasattr(pa.lib, 'PyExtensionType'):\n",
    "            print(\"âœ… PyExtensionType is available\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ PyExtensionType not found - version issue detected\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"âŒ PyArrow not installed\")\n",
    "        return False\n",
    "\n",
    "def fix_pyarrow_compatibility():\n",
    "    \"\"\"Fix PyArrow version compatibility\"\"\"\n",
    "    print(\"\\nğŸ”§ Fixing PyArrow compatibility...\")\n",
    "    \n",
    "    try:\n",
    "        # Uninstall and reinstall with specific compatible versions\n",
    "        print(\"ğŸ“¦ Uninstalling existing PyArrow...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyarrow\"])\n",
    "        \n",
    "        print(\"ğŸ“¦ Installing compatible PyArrow version...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow>=12.0.0,<15.0.0\"])\n",
    "        \n",
    "        # Also ensure pandas compatibility\n",
    "        print(\"ğŸ“¦ Updating pandas for compatibility...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pandas>=1.5.0\"])\n",
    "        \n",
    "        print(\"âœ… PyArrow compatibility fix complete!\")\n",
    "        return True\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ Failed to fix PyArrow: {e}\")\n",
    "        return False\n",
    "\n",
    "def install_package(package_name, import_name=None):\n",
    "    \"\"\"Install a package and verify it can be imported\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        # Try to import the package\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"âœ… {package_name} is already installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ Installing {package_name}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"âœ… {package_name} installed successfully\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âŒ Failed to install {package_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "print(\"ğŸ”§ Setting up BioTrove Reptilia processing environment...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, check and fix PyArrow if needed\n",
    "if not check_and_fix_pyarrow():\n",
    "    print(\"\\nğŸ› ï¸ Attempting to fix PyArrow compatibility...\")\n",
    "    if fix_pyarrow_compatibility():\n",
    "        # Re-check after fix\n",
    "        if check_and_fix_pyarrow():\n",
    "            print(\"ğŸ‰ PyArrow issue resolved!\")\n",
    "        else:\n",
    "            print(\"âš ï¸ PyArrow issue persists - may need manual intervention\")\n",
    "    else:\n",
    "        print(\"âŒ Could not automatically fix PyArrow\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# List of required packages (with specific versions to avoid conflicts)\n",
    "required_packages = [\n",
    "    (\"arbor-process\", \"arbor_process\"),\n",
    "    (\"nest_asyncio\", \"nest_asyncio\"),\n",
    "    (\"datasets>=2.14.0\", \"datasets\"),\n",
    "    (\"pandas>=1.5.0\", \"pandas\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"seaborn\", \"seaborn\"),\n",
    "    (\"pillow\", \"PIL\"),\n",
    "    (\"requests\", \"requests\"),\n",
    "    (\"tqdm\", \"tqdm\")\n",
    "]\n",
    "\n",
    "# Install packages\n",
    "all_installed = True\n",
    "for package_name, import_name in required_packages:\n",
    "    if not install_package(package_name, import_name):\n",
    "        all_installed = False\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\nğŸ‰ All packages installed successfully!\")\n",
    "    print(\"ğŸ“š Ready to process BioTrove Reptilia dataset\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Some packages failed to install\")\n",
    "    print(\"ğŸ’¡ You may need to install them manually\")\n",
    "\n",
    "print(\"\\nğŸ’¡ If you still get PyArrow errors, try:\")\n",
    "print(\"   pip install --force-reinstall pyarrow==14.0.2\")\n",
    "print(\"   pip install --force-reinstall pandas==2.0.3\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "# Import BioTrove official processing library\n",
    "try:\n",
    "    from arbor_process import *\n",
    "    print(\"âœ… BioTrove processing library (arbor_process) imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"âŒ BioTrove processing library not found!\")\n",
    "    print(\"ğŸ’¡ Please run the installation cell above first\")\n",
    "    print(\"ğŸ’¡ If that fails, try: pip install arbor-process\")\n",
    "\n",
    "# Set up environment\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ“¦ Core libraries loaded successfully!\")\n",
    "print(\"ğŸ Ready to work with BioTrove Reptilia dataset using official preprocessing tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify BioTrove processing tools are available\n",
    "print(\"ğŸ” Verifying BioTrove processing tools...\")\n",
    "\n",
    "try:\n",
    "    # Test if we can access the main BioTrove processing classes\n",
    "    from arbor_process import MetadataProcessor, GenShuffledChunks, GetImages, GenImgTxtPair, load_config\n",
    "    \n",
    "    print(\"âœ… MetadataProcessor - Available\")\n",
    "    print(\"âœ… GenShuffledChunks - Available\") \n",
    "    print(\"âœ… GetImages - Available\")\n",
    "    print(\"âœ… GenImgTxtPair - Available\")\n",
    "    print(\"âœ… load_config - Available\")\n",
    "    print(\"\\nğŸ‰ All BioTrove processing tools are ready!\")\n",
    "    print(\"ğŸ¦ Proceeding with Reptilia dataset processing...\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ BioTrove tools not available: {e}\")\n",
    "    print(\"\\nğŸ› ï¸ TROUBLESHOOTING:\")\n",
    "    print(\"1. Run the installation cell above\")\n",
    "    print(\"2. Restart the kernel and run all cells from the beginning\")\n",
    "    print(\"3. If still failing, install manually:\")\n",
    "    print(\"   pip install arbor-process\")\n",
    "    print(\"\\nğŸ’¡ You can also use the legacy streaming method as fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111aee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download metadata files from the CORRECT Hugging Face source\n",
    "print(\"ğŸ“¥ Downloading BioTrove metadata files from BGLab/BioTrove-Train...\")\n",
    "print(\"ğŸ”„ This is the correct metadata source according to the official GitHub repository\")\n",
    "\n",
    "# Create directories for metadata\n",
    "os.makedirs(\"biotrove_metadata\", exist_ok=True)\n",
    "os.makedirs(\"biotrove_processed\", exist_ok=True)\n",
    "\n",
    "def download_biotrove_metadata():\n",
    "    \"\"\"Download metadata from the correct BGLab/BioTrove-Train dataset\"\"\"\n",
    "    print(\"ğŸ“¡ Loading BioTrove metadata from Hugging Face...\")\n",
    "    \n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        # Load the correct dataset that contains the metadata\n",
    "        print(\"ğŸ” Accessing BGLab/BioTrove-Train dataset...\")\n",
    "        dataset = load_dataset(\"BGLab/BioTrove-Train\", streaming=True)\n",
    "        \n",
    "        print(\"âœ… Successfully connected to BioTrove-Train dataset!\")\n",
    "        print(\"ğŸ“Š Dataset structure:\")\n",
    "        print(f\"   Available splits: {list(dataset.keys())}\")\n",
    "        \n",
    "        # Let's examine the structure first\n",
    "        train_data = dataset['train'] if 'train' in dataset else next(iter(dataset.values()))\n",
    "        \n",
    "        # Get sample to understand structure\n",
    "        print(\"\\nğŸ” Examining dataset structure...\")\n",
    "        print(\"ğŸ’¡ Searching more extensively for Reptilia samples...\")\n",
    "        print(\"âš ï¸ Note: Dataset has 135M samples - this may take a few minutes\")\n",
    "        \n",
    "        sample_count = 0\n",
    "        reptile_samples = []\n",
    "        all_classes_seen = set()\n",
    "        check_points = [0, 50000, 100000, 500000, 1000000]  # Check different positions\n",
    "        \n",
    "        from tqdm.auto import tqdm\n",
    "        pbar = tqdm(desc=\"Scanning for reptiles\", unit=\" samples\")\n",
    "        \n",
    "        for item in train_data:\n",
    "            sample_count += 1\n",
    "            \n",
    "            # Check if this is reptile data\n",
    "            if isinstance(item, dict):\n",
    "                class_value = item.get('class', item.get('category', ''))\n",
    "                all_classes_seen.add(str(class_value))\n",
    "                \n",
    "                if class_value == 'Reptilia' or 'reptil' in str(class_value).lower():\n",
    "                    reptile_samples.append(item)\n",
    "                    if len(reptile_samples) <= 10:  # Show first few samples\n",
    "                        print(f\"\\n   ğŸ¦ Found Reptilia sample {len(reptile_samples)}: {item.get('species', 'Unknown species')}\")\n",
    "                        pbar.set_description(f\"Found {len(reptile_samples)} reptiles!\")\n",
    "            \n",
    "            # Update progress\n",
    "            if sample_count % 10000 == 0:\n",
    "                pbar.set_description(f\"Processed {sample_count:,} | Found {len(reptile_samples)} reptiles\")\n",
    "                pbar.update(10000)\n",
    "                \n",
    "                # Show what classes we've seen so far\n",
    "                if sample_count % 50000 == 0:\n",
    "                    print(f\"\\nğŸ“Š Classes seen so far ({sample_count:,} samples): {sorted(list(all_classes_seen))}\")\n",
    "            \n",
    "            # Stop if we found enough reptiles or processed enough samples\n",
    "            if len(reptile_samples) >= 1000:  # Stop after finding 1000 reptiles\n",
    "                print(f\"\\nğŸ‰ Found {len(reptile_samples)} reptiles! Stopping search.\")\n",
    "                break\n",
    "            elif sample_count >= 2000000:  # Stop after 2M samples if no reptiles found\n",
    "                print(f\"\\nâš ï¸ Searched {sample_count:,} samples without finding Reptilia\")\n",
    "                break\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        if reptile_samples:\n",
    "            print(f\"\\nğŸ‰ Found {len(reptile_samples)} Reptilia samples in {sample_count:,} records!\")\n",
    "            \n",
    "            # Create sample parquet files for processing\n",
    "            chunk_size = 500\n",
    "            saved_files = []\n",
    "            \n",
    "            for i in range(0, len(reptile_samples), chunk_size):\n",
    "                chunk = reptile_samples[i:i+chunk_size]\n",
    "                chunk_df = pd.DataFrame(chunk)\n",
    "                \n",
    "                chunk_filename = f\"biotrove_metadata/reptilia_chunk_{i//chunk_size}.parquet\"\n",
    "                chunk_df.to_parquet(chunk_filename, index=False)\n",
    "                saved_files.append(chunk_filename)\n",
    "                \n",
    "                print(f\"ğŸ’¾ Saved chunk {i//chunk_size}: {len(chunk)} samples to {chunk_filename}\")\n",
    "            \n",
    "            return saved_files\n",
    "        else:\n",
    "            print(f\"âš ï¸ No Reptilia samples found in first {sample_count:,} records\")\n",
    "            print(f\"ğŸ“Š All classes seen: {sorted(list(all_classes_seen))}\")\n",
    "            \n",
    "            # The dataset might have a different structure - let's save what we found\n",
    "            sample_data = []\n",
    "            train_data_fresh = dataset['train'] if 'train' in dataset else next(iter(dataset.values()))\n",
    "            \n",
    "            print(\"ğŸ’¡ Saving sample data for analysis...\")\n",
    "            for i, item in enumerate(train_data_fresh):\n",
    "                sample_data.append(item)\n",
    "                if i >= 500:  # Get 500 samples to analyze\n",
    "                    break\n",
    "            \n",
    "            if sample_data:\n",
    "                sample_df = pd.DataFrame(sample_data)\n",
    "                sample_file = \"biotrove_metadata/sample_structure.parquet\"\n",
    "                sample_df.to_parquet(sample_file, index=False)\n",
    "                \n",
    "                print(f\"\\nğŸ“Š Created sample file to analyze structure: {sample_file}\")\n",
    "                print(f\"   ğŸ“ Shape: {sample_df.shape}\")\n",
    "                print(f\"   ğŸ·ï¸ Columns: {list(sample_df.columns)}\")\n",
    "                \n",
    "                # Show unique values in potential class columns\n",
    "                for col in sample_df.columns:\n",
    "                    if 'class' in col.lower() or 'category' in col.lower() or 'tax' in col.lower():\n",
    "                        unique_vals = sample_df[col].unique()[:10]\n",
    "                        print(f\"   ğŸ” {col} values: {unique_vals}\")\n",
    "                \n",
    "                return [sample_file]\n",
    "            \n",
    "        return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error accessing BioTrove-Train dataset: {e}\")\n",
    "        print(\"\\nğŸ› ï¸ TROUBLESHOOTING:\")\n",
    "        print(\"1. Check internet connection\")\n",
    "        print(\"2. Verify dataset exists: https://huggingface.co/datasets/BGLab/BioTrove-Train\")\n",
    "        print(\"3. Try alternative download methods\")\n",
    "        return []\n",
    "\n",
    "def try_direct_download_from_hf():\n",
    "    \"\"\"Try downloading files directly from HuggingFace file browser\"\"\"\n",
    "    print(\"\\nğŸ”„ Trying direct file download from HuggingFace...\")\n",
    "    \n",
    "    base_url = \"https://huggingface.co/datasets/BGLab/BioTrove-Train/resolve/main/\"\n",
    "    \n",
    "    # Common file patterns to try\n",
    "    file_patterns = [\n",
    "        \"data.parquet\",\n",
    "        \"train.parquet\", \n",
    "        \"metadata.parquet\",\n",
    "        \"arboretum.parquet\",\n",
    "        \"dataset.parquet\"\n",
    "    ]\n",
    "    \n",
    "    downloaded_files = []\n",
    "    \n",
    "    for filename in file_patterns:\n",
    "        url = base_url + filename\n",
    "        filepath = f\"biotrove_metadata/{filename}\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"â¬‡ï¸  Trying to download {filename}...\")\n",
    "            \n",
    "            response = requests.get(url, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                # Verify it's a valid parquet file\n",
    "                try:\n",
    "                    test_df = pd.read_parquet(filepath)\n",
    "                    print(f\"âœ… Successfully downloaded {filename} ({len(test_df):,} rows)\")\n",
    "                    downloaded_files.append(filepath)\n",
    "                    break  # Found a good file, stop trying others\n",
    "                except:\n",
    "                    os.remove(filepath)  # Remove invalid file\n",
    "                    print(f\"âŒ {filename} is not a valid parquet file\")\n",
    "            else:\n",
    "                print(f\"âŒ {filename} not found (HTTP {response.status_code})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error downloading {filename}: {e}\")\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "# Try downloading metadata from the correct source\n",
    "print(\"ğŸš€ Starting metadata download from correct source...\")\n",
    "downloaded_files = download_biotrove_metadata()\n",
    "\n",
    "# If that fails, try direct download\n",
    "if not downloaded_files:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ”„ Fallback: Trying direct file download...\")\n",
    "    downloaded_files = try_direct_download_from_hf()\n",
    "\n",
    "if downloaded_files:\n",
    "    print(f\"\\nğŸ‰ Successfully obtained {len(downloaded_files)} metadata files!\")\n",
    "    \n",
    "    # Verify the files we downloaded\n",
    "    print(\"\\nğŸ” Analyzing downloaded metadata...\")\n",
    "    for file_path in downloaded_files:\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            print(f\"âœ… {os.path.basename(file_path)}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "            print(f\"   ğŸ·ï¸ Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Look for class/category information\n",
    "            class_columns = [col for col in df.columns if 'class' in col.lower() or 'category' in col.lower()]\n",
    "            if class_columns:\n",
    "                for col in class_columns:\n",
    "                    unique_classes = df[col].value_counts()\n",
    "                    print(f\"   ğŸ“Š {col} distribution: {dict(unique_classes.head())}\")\n",
    "                    \n",
    "                    # Check specifically for reptiles\n",
    "                    if 'Reptilia' in unique_classes.index:\n",
    "                        reptile_count = unique_classes['Reptilia']\n",
    "                        print(f\"   ğŸ¦ Found {reptile_count:,} Reptilia samples!\")\n",
    "            \n",
    "            # Look for species information  \n",
    "            if 'species' in df.columns:\n",
    "                species_count = df['species'].nunique()\n",
    "                print(f\"   ğŸ¦ Unique species: {species_count:,}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading {file_path}: {e}\")\n",
    "    \n",
    "    print(\"\\nâœ… Metadata ready for BioTrove processing!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâŒ Could not download metadata from any source\")\n",
    "    print(\"\\nğŸ› ï¸ MANUAL DOWNLOAD INSTRUCTIONS:\")\n",
    "    print(\"1. Go to: https://huggingface.co/datasets/BGLab/BioTrove-Train\") \n",
    "    print(\"2. Browse the files and download parquet files\")\n",
    "    print(\"3. Save them in the biotrove_metadata/ folder\")\n",
    "    print(\"4. Continue with processing steps\")\n",
    "    \n",
    "print(f\"\\nğŸ’¡ If no reptiles found, the dataset might be:\")\n",
    "print(\"   â€¢ Sorted by taxonomic class (Reptilia might be later)\")\n",
    "print(\"   â€¢ Very large (135M samples) - reptiles might be sparse\")\n",
    "print(\"   â€¢ Using different class names or structure\")\n",
    "print(\"   â€¢ Consider trying the comprehensive streaming approach in next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a777cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE METHOD: Strategic search through BioTrove-Train dataset\n",
    "print(\"ğŸ”„ COMPREHENSIVE METHOD: Strategic search for Reptilia in BioTrove-Train\")\n",
    "print(\"ğŸ’¡ This uses skip patterns to sample different parts of the 135M dataset\")\n",
    "\n",
    "def extract_reptilia_with_skip_strategy():\n",
    "    \"\"\"Extract Reptilia samples using skip strategy to sample the entire dataset\"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        print(\"ğŸ“¡ Loading complete BGLab/BioTrove-Train dataset...\")\n",
    "        \n",
    "        # Try different loading approaches\n",
    "        try:\n",
    "            # Use streaming for large dataset\n",
    "            dataset = load_dataset(\"BGLab/BioTrove-Train\", streaming=True)\n",
    "            is_streaming = True\n",
    "        except:\n",
    "            try:\n",
    "                # If streaming fails, try regular load\n",
    "                dataset = load_dataset(\"BGLab/BioTrove-Train\")\n",
    "                is_streaming = False\n",
    "            except:\n",
    "                raise Exception(\"Could not load dataset in any mode\")\n",
    "        \n",
    "        print(f\"âœ… Dataset loaded ({'streaming' if is_streaming else 'full'} mode)\")\n",
    "        \n",
    "        # Get the main data split\n",
    "        if 'train' in dataset:\n",
    "            data_split = dataset['train']\n",
    "        else:\n",
    "            data_split = next(iter(dataset.values()))\n",
    "        \n",
    "        reptilia_samples = []\n",
    "        total_processed = 0\n",
    "        all_classes_found = set()\n",
    "        \n",
    "        # Strategy: Sample different parts of the dataset\n",
    "        print(f\"ğŸ” Using strategic sampling to find Reptilia in 135M samples...\")\n",
    "        print(f\"ğŸ’¡ Will check samples at different intervals to cover the entire dataset\")\n",
    "        \n",
    "        from tqdm.auto import tqdm\n",
    "        pbar = tqdm(desc=\"Strategic sampling\", unit=\" samples\")\n",
    "        \n",
    "        # Skip pattern: check every Nth sample to cover more ground\n",
    "        skip_patterns = [1, 50, 500, 5000]  # Start dense, then sparse\n",
    "        max_samples_per_pattern = 50000\n",
    "        \n",
    "        for skip_size in skip_patterns:\n",
    "            print(f\"\\nğŸ¯ Checking every {skip_size} samples...\")\n",
    "            pattern_samples = 0\n",
    "            pattern_reptiles_found = 0\n",
    "            \n",
    "            sample_iter = iter(data_split)\n",
    "            \n",
    "            try:\n",
    "                while pattern_samples < max_samples_per_pattern:\n",
    "                    # Get the next item\n",
    "                    for _ in range(skip_size):\n",
    "                        try:\n",
    "                            item = next(sample_iter)\n",
    "                            total_processed += 1\n",
    "                        except StopIteration:\n",
    "                            print(f\"ğŸ“„ Reached end of dataset at {total_processed:,} samples\")\n",
    "                            break\n",
    "                    else:\n",
    "                        # Process this item\n",
    "                        if isinstance(item, dict):\n",
    "                            # Check multiple possible class/category fields\n",
    "                            item_class = None\n",
    "                            for field in ['class', 'category', 'taxonomic_class', 'tax_class']:\n",
    "                                if field in item:\n",
    "                                    item_class = str(item[field]).strip()\n",
    "                                    all_classes_found.add(item_class)\n",
    "                                    break\n",
    "                            \n",
    "                            # Check if this is a Reptilia sample\n",
    "                            if item_class and (item_class == 'Reptilia' or 'reptil' in item_class.lower()):\n",
    "                                reptilia_samples.append(item)\n",
    "                                pattern_reptiles_found += 1\n",
    "                                \n",
    "                                if len(reptilia_samples) <= 10:\n",
    "                                    species = item.get('species', item.get('scientificName', 'Unknown'))\n",
    "                                    print(f\"ğŸ¦ Found reptile #{len(reptilia_samples)}: {species}\")\n",
    "                                \n",
    "                                pbar.set_description(f\"Found {len(reptilia_samples)} Reptilia!\")\n",
    "                        \n",
    "                        pattern_samples += 1\n",
    "                        if pattern_samples % 1000 == 0:\n",
    "                            pbar.update(1000)\n",
    "                        \n",
    "                        # Stop if we found enough reptiles\n",
    "                        if len(reptilia_samples) >= 2000:\n",
    "                            print(f\"\\nğŸ‰ Found {len(reptilia_samples)} reptiles! Stopping search.\")\n",
    "                            break\n",
    "                \n",
    "                print(f\"   Pattern results: {pattern_reptiles_found} reptiles from {pattern_samples:,} samples\")\n",
    "                \n",
    "                # If we found reptiles, no need for sparser patterns\n",
    "                if len(reptilia_samples) > 0:\n",
    "                    break\n",
    "                    \n",
    "            except StopIteration:\n",
    "                print(f\"   Reached dataset end during pattern {skip_size}\")\n",
    "                break\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š SEARCH SUMMARY:\")\n",
    "        print(f\"   Total samples processed: {total_processed:,}\")\n",
    "        print(f\"   Classes found: {sorted(list(all_classes_found))}\")\n",
    "        print(f\"   Reptilia samples found: {len(reptilia_samples)}\")\n",
    "        \n",
    "        if reptilia_samples:\n",
    "            # Save reptilia samples as parquet chunks\n",
    "            chunk_size = 250\n",
    "            saved_files = []\n",
    "            \n",
    "            print(f\"\\nğŸ’¾ Saving {len(reptilia_samples)} Reptilia samples in chunks...\")\n",
    "            \n",
    "            for i in range(0, len(reptilia_samples), chunk_size):\n",
    "                chunk_data = reptilia_samples[i:i+chunk_size]\n",
    "                chunk_df = pd.DataFrame(chunk_data)\n",
    "                \n",
    "                chunk_filename = f\"biotrove_metadata/biotrove_reptilia_chunk_{i//chunk_size}.parquet\"\n",
    "                chunk_df.to_parquet(chunk_filename, index=False)\n",
    "                saved_files.append(chunk_filename)\n",
    "                \n",
    "                print(f\"âœ… Chunk {i//chunk_size}: {len(chunk_data)} samples â†’ {chunk_filename}\")\n",
    "            \n",
    "            print(f\"\\nğŸ‰ SUCCESS! Extracted {len(reptilia_samples)} Reptilia samples\")\n",
    "            print(f\"ğŸ“Š Processed {total_processed:,} total records with strategic sampling\")\n",
    "            print(f\"ğŸ’¾ Created {len(saved_files)} metadata chunks\")\n",
    "            \n",
    "            # Show sample species information\n",
    "            all_reptilia_df = pd.DataFrame(reptilia_samples)\n",
    "            if 'species' in all_reptilia_df.columns:\n",
    "                species_counts = all_reptilia_df['species'].value_counts()\n",
    "                print(f\"ğŸ¦ Found {len(species_counts)} unique reptile species\")\n",
    "                print(f\"   Top 5: {dict(species_counts.head())}\")\n",
    "            elif 'scientificName' in all_reptilia_df.columns:\n",
    "                species_counts = all_reptilia_df['scientificName'].value_counts()\n",
    "                print(f\"ğŸ¦ Found {len(species_counts)} unique reptile species\")\n",
    "                print(f\"   Top 5: {dict(species_counts.head())}\")\n",
    "            \n",
    "            return saved_files\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nâŒ No Reptilia samples found even with strategic sampling\")\n",
    "            print(f\"ğŸ’¡ The dataset might:\")\n",
    "            print(f\"   â€¢ Not contain Reptilia class\")\n",
    "            print(f\"   â€¢ Use different taxonomic naming\")\n",
    "            print(f\"   â€¢ Have reptiles under a different field\")\n",
    "            \n",
    "            # Save a larger sample to analyze structure\n",
    "            print(\"ğŸ’¾ Creating larger analysis sample...\")\n",
    "            sample_data = []\n",
    "            data_split_fresh = dataset['train'] if 'train' in dataset else next(iter(dataset.values()))\n",
    "            \n",
    "            for i, item in enumerate(data_split_fresh):\n",
    "                sample_data.append(item)\n",
    "                if i >= 2000:  # Get more samples for analysis\n",
    "                    break\n",
    "            \n",
    "            if sample_data:\n",
    "                sample_df = pd.DataFrame(sample_data)\n",
    "                sample_file = \"biotrove_metadata/biotrove_large_sample.parquet\"\n",
    "                sample_df.to_parquet(sample_file, index=False)\n",
    "                \n",
    "                print(f\"ğŸ“Š Large sample saved to: {sample_file}\")\n",
    "                print(f\"   Columns: {list(sample_df.columns)}\")\n",
    "                print(f\"   Shape: {sample_df.shape}\")\n",
    "                \n",
    "                # Detailed class analysis\n",
    "                for col in sample_df.columns:\n",
    "                    if any(keyword in col.lower() for keyword in ['class', 'category', 'tax', 'phylum']):\n",
    "                        unique_vals = sample_df[col].value_counts()\n",
    "                        print(f\"   {col} distribution: {dict(unique_vals.head(10))}\")\n",
    "                \n",
    "                return [sample_file]\n",
    "            \n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing BioTrove-Train dataset: {e}\")\n",
    "        print(\"\\nğŸ’¡ Possible issues:\")\n",
    "        print(\"   - Dataset access problems\")\n",
    "        print(\"   - Network connectivity issues\") \n",
    "        print(\"   - Dataset structure different than expected\")\n",
    "        return []\n",
    "\n",
    "# If previous methods didn't work, try strategic search\n",
    "if not downloaded_files or not any('reptilia' in f.lower() for f in downloaded_files):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ”„ Trying strategic search through BioTrove-Train dataset...\")\n",
    "    new_files = extract_reptilia_with_skip_strategy()\n",
    "    \n",
    "    # Add any new files found\n",
    "    if new_files:\n",
    "        downloaded_files.extend(new_files)\n",
    "\n",
    "print(f\"\\nğŸ FINAL RESULT: {len(downloaded_files)} metadata files available\")\n",
    "if downloaded_files:\n",
    "    # Check if any files actually contain reptiles\n",
    "    reptile_files = [f for f in downloaded_files if 'reptilia' in f.lower()]\n",
    "    if reptile_files:\n",
    "        print(f\"âœ… Found {len(reptile_files)} files with Reptilia data!\")\n",
    "        print(\"ğŸ“ Ready to proceed with BioTrove processing pipeline!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Files available but may not contain Reptilia data\")\n",
    "        print(\"ğŸ“Š Files contain sample data for structure analysis\")\n",
    "else:\n",
    "    print(\"âŒ Could not obtain any metadata files\")\n",
    "    print(\"ğŸ’¡ Consider manual download from https://huggingface.co/datasets/BGLab/BioTrove-Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for BioTrove processing focused on Reptilia\n",
    "config = {\n",
    "    \"metadata_processor_info\": {\n",
    "        \"source_folder\": \"biotrove_metadata\",\n",
    "        \"destination_folder\": \"biotrove_processed\", \n",
    "        \"categories\": [\"Reptilia\"]  # Focus only on reptile data\n",
    "    },\n",
    "    \"metadata_filter_and_shuffle_info\": {\n",
    "        \"species_count_data\": \"biotrove_processed/species_category_counts.csv\",\n",
    "        \"directory\": \"biotrove_metadata\",\n",
    "        \"rare_threshold\": 5,      # Minimum samples per species\n",
    "        \"cap_threshold\": 500,     # Maximum samples per species for balanced dataset\n",
    "        \"part_size\": 100,         # Samples per processing chunk\n",
    "        \"rare_dir\": \"biotrove_processed/rare_cases\",\n",
    "        \"cap_filtered_dir_train\": \"biotrove_processed/cap_filtered_train\", \n",
    "        \"capped_dir\": \"biotrove_processed/capped_cases\",\n",
    "        \"merged_dir\": \"biotrove_processed/merged_cases\",\n",
    "        \"files_per_chunk\": 3,\n",
    "        \"random_seed\": 42\n",
    "    },\n",
    "    \"image_download_info\": {\n",
    "        \"input_folder\": \"biotrove_processed/cap_filtered_train\",\n",
    "        \"output_folder\": \"reptile_images\", \n",
    "        \"start_index\": 0,\n",
    "        \"end_index\": 10,          # Limit for demo - increase for full processing\n",
    "        \"concurrent_downloads\": 50  # Adjust based on your internet connection\n",
    "    },\n",
    "    \"img_text_gen_info\": {\n",
    "        \"metadata\": \"biotrove_processed/cap_filtered_train\",\n",
    "        \"img_folder\": \"reptile_images\",\n",
    "        \"output_base_folder\": \"reptile_dataset\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration file\n",
    "with open('biotrove_reptilia_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"âš™ï¸ Configuration created for Reptilia processing:\")\n",
    "print(\"   ğŸ¯ Target: Reptilia class only\")  \n",
    "print(\"   ğŸ“Š Rare threshold: â‰¥5 samples per species\")\n",
    "print(\"   âš–ï¸ Cap threshold: â‰¤500 samples per species\")\n",
    "print(\"   ğŸ’¾ Config saved to: biotrove_reptilia_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Process metadata files to extract Reptilia samples\n",
    "print(\"ğŸ” Step 1: Processing metadata to extract Reptilia samples...\")\n",
    "\n",
    "# First, let's check what metadata we actually have\n",
    "metadata_dir = \"biotrove_metadata\"\n",
    "if os.path.exists(metadata_dir):\n",
    "    metadata_files = [f for f in os.listdir(metadata_dir) if f.endswith('.parquet')]\n",
    "    print(f\"ğŸ“‚ Found {len(metadata_files)} metadata files: {metadata_files}\")\n",
    "    \n",
    "    if metadata_files:\n",
    "        # Check the content of the first file\n",
    "        sample_file = os.path.join(metadata_dir, metadata_files[0])\n",
    "        sample_df = pd.read_parquet(sample_file)\n",
    "        \n",
    "        print(f\"\\nğŸ” Examining metadata structure:\")\n",
    "        print(f\"   ğŸ“ Shape: {sample_df.shape}\")\n",
    "        print(f\"   ğŸ·ï¸ Columns: {list(sample_df.columns)}\")\n",
    "        \n",
    "        # Check for class/category information\n",
    "        class_columns = [col for col in sample_df.columns if 'class' in col.lower() or 'category' in col.lower()]\n",
    "        if class_columns:\n",
    "            for col in class_columns:\n",
    "                unique_vals = sample_df[col].unique()\n",
    "                print(f\"   ğŸ“Š {col} values: {unique_vals[:10]}\")\n",
    "                \n",
    "                # Check if we have reptiles\n",
    "                reptile_mask = sample_df[col].astype(str).str.contains('reptil', case=False, na=False)\n",
    "                reptile_count = reptile_mask.sum()\n",
    "                if reptile_count > 0:\n",
    "                    print(f\"   ğŸ¦ Found {reptile_count} Reptilia samples!\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ No Reptilia samples found in {col}\")\n",
    "        \n",
    "        # Check if this looks like actual BioTrove data or just a sample\n",
    "        if sample_df.shape[0] <= 1000 and 'sample_structure' in metadata_files[0]:\n",
    "            print(f\"\\nâš ï¸ WARNING: This appears to be a sample/structure file, not actual reptile metadata\")\n",
    "            print(f\"ğŸ’¡ You need to download actual Reptilia data from BGLab/BioTrove-Train\")\n",
    "            print(f\"ğŸ“ Suggestion: Re-run the metadata download cells (7-8) to get real reptile data\")\n",
    "        \n",
    "        has_reptiles = any(\n",
    "            sample_df[col].astype(str).str.contains('reptil', case=False, na=False).any() \n",
    "            for col in class_columns\n",
    "        )\n",
    "        \n",
    "        if not has_reptiles:\n",
    "            print(f\"\\nâŒ No Reptilia data available for processing\")\n",
    "            print(f\"ğŸ› ï¸ SOLUTIONS:\")\n",
    "            print(f\"   1. Re-run cells 7-8 to download actual reptile metadata\")\n",
    "            print(f\"   2. Check internet connection and dataset access\")\n",
    "            print(f\"   3. Consider using the streaming approach in cell 8\")\n",
    "\n",
    "try:\n",
    "    # Only proceed if we have metadata files and reptiles\n",
    "    if metadata_files and has_reptiles:\n",
    "        # Load configuration\n",
    "        config = load_config('biotrove_reptilia_config.json')\n",
    "        \n",
    "        # Initialize MetadataProcessor\n",
    "        params = config.get('metadata_processor_info', {})\n",
    "        mp = MetadataProcessor(**params)\n",
    "        \n",
    "        print(\"\\nğŸ“Š Processing metadata files with BioTrove tools...\")\n",
    "        mp.process_all_files()\n",
    "        \n",
    "        print(\"âœ… Metadata processing complete!\")\n",
    "        \n",
    "        # Check results\n",
    "        species_counts_file = \"biotrove_processed/species_category_counts.csv\"\n",
    "        if os.path.exists(species_counts_file):\n",
    "            species_counts = pd.read_csv(species_counts_file)\n",
    "            print(f\"ğŸ“ˆ Found {len(species_counts)} species records\")\n",
    "            \n",
    "            # Show reptile species if any found\n",
    "            if 'category' in species_counts.columns:\n",
    "                reptile_species = species_counts[species_counts['category'] == 'Reptilia']\n",
    "            else:\n",
    "                # Try alternative column names\n",
    "                alt_cols = [col for col in species_counts.columns if 'class' in col.lower()]\n",
    "                if alt_cols:\n",
    "                    col_name = alt_cols[0]\n",
    "                    reptile_species = species_counts[species_counts[col_name] == 'Reptilia']\n",
    "                else:\n",
    "                    reptile_species = pd.DataFrame()\n",
    "            \n",
    "            if len(reptile_species) > 0:\n",
    "                print(f\"ğŸ¦ Reptilia species found: {len(reptile_species)}\")\n",
    "                print(\"ğŸ† Top 5 Reptilia species by count:\")\n",
    "                \n",
    "                # Ensure count column is numeric\n",
    "                count_col = 'count'\n",
    "                if count_col in reptile_species.columns:\n",
    "                    reptile_species[count_col] = pd.to_numeric(reptile_species[count_col], errors='coerce')\n",
    "                    top_species = reptile_species.nlargest(5, count_col)\n",
    "                    for _, row in top_species.iterrows():\n",
    "                        species_name = row.get('species', 'Unknown')\n",
    "                        count_val = row.get(count_col, 0)\n",
    "                        print(f\"   {species_name}: {count_val} samples\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ Count column not found in results\")\n",
    "            else:\n",
    "                print(\"âš ï¸ No Reptilia samples found in the processed chunks\")\n",
    "                print(\"ğŸ’¡ This confirms we need actual reptile metadata, not sample data\")\n",
    "        else:\n",
    "            print(f\"âŒ Species count file not created: {species_counts_file}\")\n",
    "            print(\"ğŸ’¡ The metadata processing failed - likely due to wrong data format\")\n",
    "    else:\n",
    "        print(\"âŒ No metadata files found to process\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in metadata processing: {str(e)}\")\n",
    "    print(\"ğŸ’¡ Common issues:\")\n",
    "    print(\"   â€¢ Wrong data format (need actual BioTrove metadata, not sample files)\")\n",
    "    print(\"   â€¢ Missing or incompatible columns\")\n",
    "    print(\"   â€¢ Data type mismatches\")\n",
    "    print(\"   â€¢ Ensure arbor_process is properly installed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Step 2: Filter and shuffle the data\n",
    "print(\"ğŸ”„ Step 2: Filtering and shuffling Reptilia data...\")\n",
    "\n",
    "try:\n",
    "    # Check if the species count file exists first\n",
    "    species_counts_file = \"biotrove_processed/species_category_counts.csv\"\n",
    "    if not os.path.exists(species_counts_file):\n",
    "        print(\"âŒ Cannot proceed: species_category_counts.csv not found\")\n",
    "        print(\"ğŸ’¡ Step 1 must complete successfully before running Step 2\")\n",
    "        print(\"ğŸ› ï¸ Please fix the metadata issues in Step 1 first\")\n",
    "    else:\n",
    "        config = load_config('biotrove_reptilia_config.json')\n",
    "        params = config.get('metadata_filter_and_shuffle_info', {})\n",
    "        gen_shuffled_chunks = GenShuffledChunks(**params)\n",
    "        \n",
    "        print(\"âš–ï¸ Applying rare/cap thresholds and shuffling...\")\n",
    "        gen_shuffled_chunks.process_files()\n",
    "        \n",
    "        print(\"âœ… Data filtering and shuffling complete!\")\n",
    "        \n",
    "        # Check filtered results\n",
    "        cap_filtered_dir = params.get('cap_filtered_dir_train', 'biotrove_processed/cap_filtered_train')\n",
    "        if os.path.exists(cap_filtered_dir):\n",
    "            filtered_files = [f for f in os.listdir(cap_filtered_dir) if f.endswith('.parquet')]\n",
    "            print(f\"ğŸ“‚ Created {len(filtered_files)} filtered data chunks\")\n",
    "            \n",
    "            if filtered_files:\n",
    "                # Sample one file to see content\n",
    "                sample_file = os.path.join(cap_filtered_dir, filtered_files[0])\n",
    "                sample_df = pd.read_parquet(sample_file)\n",
    "                print(f\"ğŸ“Š Sample chunk contains {len(sample_df)} records\")\n",
    "                if len(sample_df) > 0:\n",
    "                    if 'species' in sample_df.columns:\n",
    "                        print(f\"ğŸ·ï¸ Sample species: {sample_df['species'].unique()[:3]}\")\n",
    "                    else:\n",
    "                        print(f\"ğŸ·ï¸ Sample columns: {list(sample_df.columns)}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Filtered data directory not created\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in filtering/shuffling: {str(e)}\")\n",
    "    print(\"ğŸ’¡ Ensure Step 1 completed successfully\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Metadata processing pipeline complete!\")\n",
    "print(\"\\nğŸ“ NEXT STEPS:\")\n",
    "if os.path.exists(\"biotrove_processed/cap_filtered_train\"):\n",
    "    print(\"   âœ… Ready to run image download step!\")\n",
    "else:\n",
    "    print(\"   âŒ Need to get actual Reptilia metadata first\")\n",
    "    print(\"   ğŸ”„ Re-run cells 7-8 to download real reptile data from BGLab/BioTrove-Train\")\n",
    "    print(\"   ğŸ“Š Current data appears to be sample/structure data only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60882258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the processed reptilia data\n",
    "def analyze_processed_reptilia_data(processed_dir=\"biotrove_processed\"):\n",
    "    \"\"\"Analyze the processed reptilia dataset\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” Analyzing processed Reptilia dataset...\")\n",
    "    \n",
    "    # Check species counts\n",
    "    species_file = os.path.join(processed_dir, \"species_category_counts.csv\")\n",
    "    if os.path.exists(species_file):\n",
    "        df = pd.read_csv(species_file)\n",
    "        reptile_data = df[df['category'] == 'Reptilia']\n",
    "        \n",
    "        if len(reptile_data) > 0:\n",
    "            print(f\"\\nğŸ“Š REPTILIA DATASET SUMMARY\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Total reptile species: {len(reptile_data)}\")\n",
    "            print(f\"Total reptile samples: {reptile_data['count'].sum():,}\")\n",
    "            print(f\"Average samples per species: {reptile_data['count'].mean():.1f}\")\n",
    "            print(f\"Median samples per species: {reptile_data['count'].median():.1f}\")\n",
    "            \n",
    "            # Top families analysis\n",
    "            if 'family' in reptile_data.columns:\n",
    "                family_counts = reptile_data.groupby('family')['count'].sum().sort_values(ascending=False)\n",
    "                print(f\"\\nğŸ† Top 5 Reptile Families:\")\n",
    "                for family, count in family_counts.head().items():\n",
    "                    print(f\"   {family}: {count:,} samples\")\n",
    "            \n",
    "            # Species distribution\n",
    "            print(f\"\\nğŸ“ˆ Species Distribution:\")\n",
    "            print(f\"   Species with >100 samples: {(reptile_data['count'] > 100).sum()}\")\n",
    "            print(f\"   Species with 50-100 samples: {((reptile_data['count'] >= 50) & (reptile_data['count'] <= 100)).sum()}\")\n",
    "            print(f\"   Species with 10-49 samples: {((reptile_data['count'] >= 10) & (reptile_data['count'] < 50)).sum()}\")\n",
    "            print(f\"   Species with <10 samples: {(reptile_data['count'] < 10).sum()}\")\n",
    "            \n",
    "            return reptile_data\n",
    "        else:\n",
    "            print(\"âŒ No Reptilia data found in processed results\")\n",
    "    else:\n",
    "        print(f\"âŒ Species count file not found: {species_file}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Run analysis\n",
    "reptile_analysis = analyze_processed_reptilia_data()\n",
    "\n",
    "# Check filtered data chunks\n",
    "cap_filtered_dir = \"biotrove_processed/cap_filtered_train\"\n",
    "if os.path.exists(cap_filtered_dir):\n",
    "    filtered_files = [f for f in os.listdir(cap_filtered_dir) if f.endswith('.parquet')]\n",
    "    print(f\"\\nğŸ“‚ Filtered data chunks available: {len(filtered_files)}\")\n",
    "    \n",
    "    if filtered_files:\n",
    "        print(\"ğŸ“‹ Ready for image download!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No filtered chunks found - may need more metadata or different thresholds\")\n",
    "else:\n",
    "    print(\"âš ï¸ Filtered data directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca38f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download reptilia images using official BioTrove tools\n",
    "print(\"ğŸ“¥ Step 3: Downloading Reptilia images...\")\n",
    "\n",
    "async def download_reptilia_images():\n",
    "    \"\"\"Download reptile images using the official BioTrove image downloader\"\"\"\n",
    "    try:\n",
    "        # Load configuration\n",
    "        config = load_config('biotrove_reptilia_config.json')\n",
    "        params = config.get('image_download_info', {})\n",
    "        \n",
    "        # Initialize image downloader\n",
    "        gi = GetImages(**params)\n",
    "        \n",
    "        print(f\"ğŸ–¼ï¸ Starting image download...\")\n",
    "        print(f\"   ğŸ“‚ Input folder: {params['input_folder']}\")\n",
    "        print(f\"   ğŸ’¾ Output folder: {params['output_folder']}\")\n",
    "        print(f\"   ğŸ“Š Processing chunks {params['start_index']} to {params['end_index']}\")\n",
    "        print(f\"   ğŸ”„ Concurrent downloads: {params['concurrent_downloads']}\")\n",
    "        print()\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(params['output_folder'], exist_ok=True)\n",
    "        \n",
    "        # Download images\n",
    "        await gi.download_images()\n",
    "        \n",
    "        print(\"âœ… Image download complete!\")\n",
    "        \n",
    "        # Check results\n",
    "        output_folder = params['output_folder']\n",
    "        if os.path.exists(output_folder):\n",
    "            # Count downloaded images\n",
    "            total_images = 0\n",
    "            species_folders = []\n",
    "            \n",
    "            for item in os.listdir(output_folder):\n",
    "                item_path = os.path.join(output_folder, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    species_folders.append(item)\n",
    "                    images_in_folder = len([f for f in os.listdir(item_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                    total_images += images_in_folder\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Download Results:\")\n",
    "            print(f\"   ğŸ–¼ï¸ Total images downloaded: {total_images}\")\n",
    "            print(f\"   ğŸ¦ Species folders created: {len(species_folders)}\")\n",
    "            \n",
    "            if species_folders:\n",
    "                print(f\"   ğŸ“‚ Sample species folders: {species_folders[:5]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error downloading images: {str(e)}\")\n",
    "        print(\"ğŸ’¡ Check that filtered data exists and internet connection is stable\")\n",
    "\n",
    "# Note: For Jupyter, we need to handle async differently\n",
    "print(\"â³ Starting image download process...\")\n",
    "print(\"ğŸ’¡ This may take several minutes depending on the number of images...\")\n",
    "\n",
    "# Check if we have filtered data to download\n",
    "cap_filtered_dir = \"biotrove_processed/cap_filtered_train\" \n",
    "if os.path.exists(cap_filtered_dir):\n",
    "    filtered_files = [f for f in os.listdir(cap_filtered_dir) if f.endswith('.parquet')]\n",
    "    if filtered_files:\n",
    "        print(f\"ğŸ“‚ Found {len(filtered_files)} filtered data chunks to process\")\n",
    "        \n",
    "        # Run the async download\n",
    "        try:\n",
    "            # For Jupyter compatibility\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            await download_reptilia_images()\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸ nest_asyncio not available. Installing...\")\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nest_asyncio\"])\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            await download_reptilia_images()\n",
    "    else:\n",
    "        print(\"âŒ No filtered data chunks found. Run the metadata processing steps first.\")\n",
    "else:\n",
    "    print(\"âŒ Filtered data directory not found. Run the metadata processing steps first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate text labels and create final dataset\n",
    "print(\"ğŸ“ Step 4: Generating text labels for Reptilia images...\")\n",
    "\n",
    "def generate_reptilia_text_labels():\n",
    "    \"\"\"Generate text labels for downloaded reptile images\"\"\"\n",
    "    try:\n",
    "        # Load configuration\n",
    "        config = load_config('biotrove_reptilia_config.json')\n",
    "        params = config.get('img_text_gen_info', {})\n",
    "        \n",
    "        # Initialize text generator\n",
    "        textgen = GenImgTxtPair(**params)\n",
    "        \n",
    "        print(\"ğŸ·ï¸ Creating image-text pairs...\")\n",
    "        print(f\"   ğŸ“‚ Metadata source: {params['metadata']}\")\n",
    "        print(f\"   ğŸ–¼ï¸ Image folder: {params['img_folder']}\")\n",
    "        print(f\"   ğŸ’¾ Output folder: {params['output_base_folder']}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(params['output_base_folder'], exist_ok=True)\n",
    "        \n",
    "        # Generate image-text pairs\n",
    "        textgen.create_image_text_pairs()\n",
    "        \n",
    "        print(\"âœ… Text label generation complete!\")\n",
    "        \n",
    "        # Check results\n",
    "        output_folder = params['output_base_folder']\n",
    "        if os.path.exists(output_folder):\n",
    "            # Count generated files\n",
    "            tar_files = [f for f in os.listdir(output_folder) if f.endswith('.tar')]\n",
    "            txt_files = []\n",
    "            json_files = []\n",
    "            \n",
    "            # Check subfolders for text files\n",
    "            for item in os.listdir(output_folder):\n",
    "                item_path = os.path.join(output_folder, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    txt_files.extend([f for f in os.listdir(item_path) if f.endswith('.txt')])\n",
    "                    json_files.extend([f for f in os.listdir(item_path) if f.endswith('.json')])\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Generated Dataset:\")\n",
    "            print(f\"   ğŸ“¦ TAR files: {len(tar_files)}\")\n",
    "            print(f\"   ğŸ“ Text files: {len(txt_files)}\")\n",
    "            print(f\"   ğŸ“„ JSON files: {len(json_files)}\")\n",
    "            \n",
    "            # Show sample text labels if available\n",
    "            if txt_files:\n",
    "                sample_txt_path = None\n",
    "                for item in os.listdir(output_folder):\n",
    "                    item_path = os.path.join(output_folder, item)\n",
    "                    if os.path.isdir(item_path):\n",
    "                        txt_files_in_dir = [f for f in os.listdir(item_path) if f.endswith('.txt')]\n",
    "                        if txt_files_in_dir:\n",
    "                            sample_txt_path = os.path.join(item_path, txt_files_in_dir[0])\n",
    "                            break\n",
    "                \n",
    "                if sample_txt_path and os.path.exists(sample_txt_path):\n",
    "                    print(f\"\\nğŸ“ Sample text label:\")\n",
    "                    with open(sample_txt_path, 'r') as f:\n",
    "                        sample_text = f.read().strip()\n",
    "                        print(f\"   '{sample_text[:100]}...'\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating text labels: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Generate text labels\n",
    "if generate_reptilia_text_labels():\n",
    "    print(\"\\nğŸ‰ BIOTROVE REPTILIA PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ… Your reptile dataset is ready for computer vision tasks!\")\n",
    "    print(\"\\nğŸ“ Dataset Structure:\")\n",
    "    print(\"   ğŸ“‚ biotrove_metadata/          - Raw metadata chunks\")\n",
    "    print(\"   ğŸ“‚ biotrove_processed/         - Processed species data\") \n",
    "    print(\"   ğŸ“‚ reptile_images/             - Downloaded reptile images\")\n",
    "    print(\"   ğŸ“‚ reptile_dataset/            - Final image-text pairs\")\n",
    "    print(\"\\nğŸ”¬ Ready for:\")\n",
    "    print(\"   â€¢ CNN training and fine-tuning\")\n",
    "    print(\"   â€¢ Vision Transformer experiments\") \n",
    "    print(\"   â€¢ YOLOv10 object detection\")\n",
    "    print(\"   â€¢ BioTrove-CLIP model evaluation\")\n",
    "    print(\"   â€¢ Conservation and biodiversity applications\")\n",
    "else:\n",
    "    print(\"âš ï¸ Text label generation failed. Check previous steps.\")\n",
    "\n",
    "# Summary statistics\n",
    "def show_final_dataset_summary():\n",
    "    \"\"\"Show final summary of the created dataset\"\"\"\n",
    "    print(f\"\\nğŸ“Š FINAL DATASET SUMMARY\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Count images\n",
    "    img_folder = \"reptile_images\"\n",
    "    total_images = 0\n",
    "    species_count = 0\n",
    "    \n",
    "    if os.path.exists(img_folder):\n",
    "        for item in os.listdir(img_folder):\n",
    "            item_path = os.path.join(img_folder, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                species_count += 1\n",
    "                images = [f for f in os.listdir(item_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                total_images += len(images)\n",
    "    \n",
    "    # Count processed species\n",
    "    species_file = \"biotrove_processed/species_category_counts.csv\"\n",
    "    total_species_in_metadata = 0\n",
    "    if os.path.exists(species_file):\n",
    "        df = pd.read_csv(species_file)\n",
    "        reptile_data = df[df['category'] == 'Reptilia']\n",
    "        total_species_in_metadata = len(reptile_data)\n",
    "    \n",
    "    print(f\"ğŸ¦ Reptile species in metadata: {total_species_in_metadata}\")\n",
    "    print(f\"ğŸ“‚ Species with downloaded images: {species_count}\")\n",
    "    print(f\"ğŸ–¼ï¸ Total reptile images: {total_images}\")\n",
    "    \n",
    "    # Show dataset is ready for herpeton project goals\n",
    "    if total_images > 0:\n",
    "        print(f\"\\nğŸ¯ READY FOR HERPETON PROJECT:\")\n",
    "        print(f\"   âœ… Species classification\")\n",
    "        print(f\"   âœ… Family-level taxonomy\")\n",
    "        print(f\"   âœ… Transfer learning experiments\")\n",
    "        print(f\"   âœ… Conservation applications\")\n",
    "\n",
    "show_final_dataset_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fab052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore and visualize the reptilia dataset\n",
    "def explore_reptile_dataset():\n",
    "    \"\"\"Explore the processed reptile dataset\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” Exploring Reptile Dataset...\")\n",
    "    \n",
    "    # Load species data\n",
    "    species_file = \"biotrove_processed/species_category_counts.csv\"\n",
    "    if not os.path.exists(species_file):\n",
    "        print(\"âŒ Species data not found. Run processing steps first.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(species_file)\n",
    "    reptile_data = df[df['category'] == 'Reptilia'].copy()\n",
    "    \n",
    "    if len(reptile_data) == 0:\n",
    "        print(\"âŒ No reptile data found.\")\n",
    "        return\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Top species\n",
    "    top_species = reptile_data.nlargest(10, 'count')\n",
    "    top_species.plot(x='species', y='count', kind='bar', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('Top 10 Reptile Species by Sample Count')\n",
    "    axes[0,0].set_xlabel('Species')\n",
    "    axes[0,0].set_ylabel('Sample Count')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Family analysis if available\n",
    "    if 'family' in reptile_data.columns:\n",
    "        family_counts = reptile_data.groupby('family')['count'].sum().sort_values(ascending=False)\n",
    "        family_counts.head(8).plot(kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "        axes[0,1].set_title('Top 8 Reptile Families')\n",
    "        axes[0,1].set_xlabel('Family')\n",
    "        axes[0,1].set_ylabel('Total Samples')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Sample distribution\n",
    "    reptile_data['count'].hist(bins=20, ax=axes[1,0], color='lightgreen', alpha=0.7)\n",
    "    axes[1,0].set_title('Distribution of Samples per Species')\n",
    "    axes[1,0].set_xlabel('Samples per Species')\n",
    "    axes[1,0].set_ylabel('Number of Species')\n",
    "    axes[1,0].set_yscale('log')\n",
    "    \n",
    "    # Cumulative coverage\n",
    "    sorted_counts = reptile_data['count'].sort_values(ascending=False)\n",
    "    cumsum_pct = (sorted_counts.cumsum() / sorted_counts.sum() * 100)\n",
    "    axes[1,1].plot(range(len(cumsum_pct)), cumsum_pct, color='purple')\n",
    "    axes[1,1].set_title('Cumulative Species Coverage')\n",
    "    axes[1,1].set_xlabel('Species Rank')\n",
    "    axes[1,1].set_ylabel('Cumulative % of Samples')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return reptile_data\n",
    "\n",
    "# Run exploration\n",
    "reptile_stats = explore_reptile_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691af7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_reptilia_dataset(save_path=\"full_reptilia_dataset.csv\", batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process the entire BioTrove dataset to extract all Reptilia samples.\n",
    "    This function is designed to run for a long time and save progress incrementally.\n",
    "    \n",
    "    Args:\n",
    "        save_path (str): Path to save the complete dataset\n",
    "        batch_size (int): Number of samples to process before saving progress\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”¥ Starting full Reptilia dataset creation...\")\n",
    "    print(\"âš ï¸  WARNING: This will take several hours to complete!\")\n",
    "    print(\"ğŸ’¡ You can stop anytime with Ctrl+C and resume later\")\n",
    "    \n",
    "    # Check if we're resuming from a previous run\n",
    "    start_idx = 0\n",
    "    existing_data = []\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        existing_df = pd.read_csv(save_path)\n",
    "        existing_data = existing_df.to_dict('records')\n",
    "        print(f\"ğŸ“‚ Resuming from existing file with {len(existing_data):,} samples\")\n",
    "    \n",
    "    # Load dataset in streaming mode\n",
    "    dataset = load_dataset(\"BGLab/BioTrove-Train\", streaming=True)\n",
    "    train_data = dataset['train']\n",
    "    \n",
    "    all_reptilia_data = existing_data.copy()\n",
    "    \n",
    "    try:\n",
    "        reptilia_count = len(existing_data)\n",
    "        total_processed = 0\n",
    "        \n",
    "        print(\"\\nğŸš€ Processing dataset...\")\n",
    "        pbar = tqdm(desc=\"Searching for reptiles\", unit=\" samples\")\n",
    "        \n",
    "        for item in train_data:\n",
    "            total_processed += 1\n",
    "            \n",
    "            # Check if this is a reptilia sample\n",
    "            if item.get('class') == 'Reptilia':\n",
    "                all_reptilia_data.append(item)\n",
    "                reptilia_count += 1\n",
    "            \n",
    "            # Update progress every 1000 samples\n",
    "            if total_processed % 1000 == 0:\n",
    "                pbar.set_description(f\"Processed {total_processed:,} | Found {reptilia_count:,} reptiles\")\n",
    "                pbar.update(1000)\n",
    "            \n",
    "            # Save progress every batch_size samples\n",
    "            if total_processed % batch_size == 0 and reptilia_count > len(existing_data):\n",
    "                temp_df = pd.DataFrame(all_reptilia_data)\n",
    "                temp_df.to_csv(save_path, index=False)\n",
    "                print(f\"\\nğŸ’¾ Progress saved: {reptilia_count:,} reptiles from {total_processed:,} samples\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\nâ¹ï¸ Processing interrupted by user\")\n",
    "        \n",
    "    finally:\n",
    "        pbar.close()\n",
    "        \n",
    "        # Final save\n",
    "        if all_reptilia_data:\n",
    "            final_df = pd.DataFrame(all_reptilia_data)\n",
    "            final_df.to_csv(save_path, index=False)\n",
    "            \n",
    "            print(f\"\\nğŸ‰ FINAL RESULTS:\")\n",
    "            print(f\"ğŸ“Š Total samples processed: {total_processed:,}\")\n",
    "            print(f\"ğŸ¦ Total reptiles found: {len(all_reptilia_data):,}\")\n",
    "            print(f\"ğŸ”¢ Unique species: {final_df['species'].nunique():,}\")\n",
    "            print(f\"ğŸ’¾ Saved to: {save_path}\")\n",
    "            \n",
    "            return final_df\n",
    "        else:\n",
    "            print(\"âŒ No data collected\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "print(\"âš¡ Full dataset processing function ready!\")\n",
    "print(\"ğŸ’¡ Call create_full_reptilia_dataset() to start processing the complete dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get species for specific families\n",
    "def get_species_by_family(df, family_name):\n",
    "    \"\"\"\n",
    "    Get all species from a specific reptile family.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Reptilia dataset\n",
    "        family_name (str): Name of the family (e.g., 'Viperidae', 'Gekkonidae')\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Species information for the family\n",
    "    \"\"\"\n",
    "    family_data = df[df['family'] == family_name]\n",
    "    \n",
    "    if len(family_data) == 0:\n",
    "        print(f\"âŒ No species found in family: {family_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    species_summary = family_data.groupby(['species', 'common_name']).size().reset_index()\n",
    "    species_summary.columns = ['Species', 'Common Name', 'Sample Count']\n",
    "    species_summary = species_summary.sort_values('Sample Count', ascending=False)\n",
    "    \n",
    "    print(f\"ğŸ Family: {family_name}\")\n",
    "    print(f\"ğŸ“Š Total species: {len(species_summary)}\")\n",
    "    print(f\"ğŸ“Š Total samples: {len(family_data):,}\")\n",
    "    \n",
    "    return species_summary\n",
    "\n",
    "# Example: Create a balanced dataset\n",
    "def create_balanced_subset(df, min_samples=10, max_samples=100):\n",
    "    \"\"\"\n",
    "    Create a balanced subset with similar number of samples per species.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Full reptilia dataset\n",
    "        min_samples (int): Minimum samples required per species\n",
    "        max_samples (int): Maximum samples to keep per species\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Balanced subset\n",
    "    \"\"\"\n",
    "    species_counts = df['species'].value_counts()\n",
    "    \n",
    "    # Filter species with sufficient samples\n",
    "    valid_species = species_counts[species_counts >= min_samples].index\n",
    "    print(f\"ğŸ“Š Species with â‰¥{min_samples} samples: {len(valid_species)}/{len(species_counts)}\")\n",
    "    \n",
    "    # Sample from each valid species\n",
    "    balanced_data = []\n",
    "    for species in valid_species:\n",
    "        species_data = df[df['species'] == species]\n",
    "        sample_size = min(max_samples, len(species_data))\n",
    "        sampled = species_data.sample(n=sample_size, random_state=42)\n",
    "        balanced_data.append(sampled)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"âœ… Created balanced dataset:\")\n",
    "    print(f\"   ğŸ“Š Total samples: {len(balanced_df):,}\")\n",
    "    print(f\"   ğŸ¦ Species included: {balanced_df['species'].nunique()}\")\n",
    "    print(f\"   ğŸ“ˆ Samples per species: {min_samples}-{max_samples}\")\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "print(\"âœ… Utility functions created!\")\n",
    "print(\"\\nğŸ“š Available functions:\")\n",
    "print(\"  â€¢ get_species_by_family(df, family_name)\")\n",
    "print(\"  â€¢ create_balanced_subset(df, min_samples, max_samples)\")\n",
    "print(\"  â€¢ show_species_samples(df, species_name, max_images)\")\n",
    "print(\"  â€¢ create_full_reptilia_dataset() - for complete dataset extraction\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
